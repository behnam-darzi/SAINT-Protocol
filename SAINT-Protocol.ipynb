{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8417e8e-af84-4849-a293-96764cf9278e",
   "metadata": {},
   "source": [
    "\n",
    "### üß† SAINT Protocol ‚Äì fMRI-Guided DLPFC Targeting Pipeline\n",
    "\n",
    "This pipeline implements the **SAINT (Stanford Accelerated Intelligent Neuromodulation Therapy)** protocol for identifying an optimal left DLPFC target for TMS based on individual resting-state fMRI connectivity with the sgACC.\n",
    "\n",
    "The workflow consists of **three sequential steps**, designed to be **modular, reproducible, and method-agnostic** (supports multiple correlation metrics).\n",
    "\n",
    "---\n",
    "\n",
    "### üî¢ Pipeline Overview\n",
    "\n",
    "| Step | Name | Purpose |\n",
    "|------|------|--------|\n",
    "| **üß± Step 0** | Environment Setup | Installs missing dependencies (`nibabel`, `plotly`, `kaleido`, etc.) |\n",
    "| **üî¨ Step 1** | Functional Parcellation | Parcellates left DLPFC and bilateral sgACC into functional subunits using HAC |\n",
    "| **üîç Step 2** | Optimal Subunit Selection | Ranks DLPFC subunits by anticorrelation, spatial concentration, and size |\n",
    "| **üñºÔ∏è Step 3** | 3D Visualization | Renders the top DLPFC subunit as a smooth mesh with anatomical context |\n",
    "\n",
    "> ‚ö†Ô∏è **Execution Order Matters**:  \n",
    "> Run **Step 1 ‚Üí Step 2 ‚Üí Step 3** in sequence. Each step depends on the outputs of the previous one.\n",
    "\n",
    "---\n",
    "\n",
    "### üìÇ Required Directory Structure\n",
    "\n",
    "Your data **must** be organized as follows:\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "21322b59-8f2e-4043-a6d1-16dda6ef3b0a",
   "metadata": {},
   "source": [
    "your_base_directory/\n",
    "‚îî‚îÄ‚îÄ Subjects/\n",
    "    ‚îú‚îÄ‚îÄ sub1/\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ filtered_func_data.nii.gz\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ l_DLPFC_bin.nii.gz (or l_DLPFC_func.nii.gz)\n",
    "    ‚îÇ   ‚îî‚îÄ‚îÄ sgACC_bin.nii.gz (or sgACC_func.nii.gz)\n",
    "    ‚îú‚îÄ‚îÄ sub2/\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ filtered_func_data.nii.gz\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ l_DLPFC_bin.nii.gz\n",
    "    ‚îÇ   ‚îî‚îÄ‚îÄ sgACC_bin.nii.gz\n",
    "    ‚îî‚îÄ‚îÄ ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de9c5cc-1920-4ffd-a561-0f8bf460c882",
   "metadata": {},
   "source": [
    "\n",
    "#### üîë Naming Rules:\n",
    "- Subject folders **must** be named `sub1`, `sub2`, `sub005`, etc. (prefix `sub` + number).\n",
    "- **Mandatory fMRI file**: `filtered_func_data.nii.gz` in each subject folder.\n",
    "- **ROI masks**: At least one of the following for each ROI:\n",
    "  - `l_DLPFC_bin.nii.gz` or `l_DLPFC_func.nii.gz`\n",
    "  - `sgACC_bin.nii.gz` or `sgACC_func.nii.gz`\n",
    "\n",
    "> üí° **Tip**: The pipeline automatically prefers `_bin.nii.gz` over `_func.nii.gz` if both exist.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è How to Configure\n",
    "\n",
    "1. **Set your base directory** at the top of **Step 1** (`Algorithm 1`):\n",
    "   ```python\n",
    "   base_dir = \"G:/SAINT/Subjects\"  # üëà CHANGE THIS TO YOUR PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03d3716-a16f-4bea-8186-2085aec0fc25",
   "metadata": {},
   "source": [
    "‚úÖ Output Files (per subject)  \n",
    "After running all steps, you‚Äôll find:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "852ad1d2-ca48-45e5-acba-00b20d24cec9",
   "metadata": {},
   "source": [
    "sub1/\n",
    "‚îî‚îÄ‚îÄ SAINT-PROTOCOL/\n",
    "    ‚îú‚îÄ‚îÄ sub1_SAINT_Protocol_Output_{method}.xlsx\n",
    "    ‚îú‚îÄ‚îÄ sub1_cluster_voxel_coords_{method}.npz\n",
    "    ‚îî‚îÄ‚îÄ sub1_top_dlpfc_cluster_{method}.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5dee78-241d-405c-9d18-d908f8fcb687",
   "metadata": {},
   "source": [
    "### üß± Step 0: Environment Setup ‚Äî Dependency Check & Installation\n",
    "\n",
    "Before running any part of the SAINT pipeline, ensure all required Python packages are installed. This step:\n",
    "\n",
    "- **Checks** which packages are already present on your system.\n",
    "- **Installs only the missing ones** (does not upgrade existing packages).\n",
    "- Uses your current Python environment (respects virtual environments).\n",
    "\n",
    "> ‚úÖ This is a **safe, non-destructive** setup step. It will not modify already-installed packages.\n",
    "\n",
    "#### üì¶ Packages Managed:\n",
    "- `nibabel` ‚Äì for neuroimaging data I/O  \n",
    "- `numpy` ‚Äì core numerical computing  \n",
    "- `pandas` ‚Äì data structures and Excel handling  \n",
    "- `scipy` ‚Äì statistical tests and clustering  \n",
    "- `plotly` ‚Äì 3D interactive visualization  \n",
    "- `scikit-image` ‚Äì `marching_cubes` for mesh generation  \n",
    "- `kaleido` ‚Äì required for saving Plotly figures as PNG/SVG  \n",
    "\n",
    "#### ‚ñ∂Ô∏è How to Use:\n",
    "Simply run this cell **once** at the beginning of your workflow. If all packages are present, it does nothing. If any are missing, it installs them silently via `pip`.\n",
    "\n",
    "> ‚ö†Ô∏è **Note**: Write access to your Python environment is required. If you encounter permission errors, run with appropriate privileges or in a virtual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ada59232-1b8e-476d-840c-293a4b1d9937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All packages already installed.\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import subprocess\n",
    "import sys\n",
    "def get_installed_versions():\n",
    "    \"\"\"\n",
    "    Returns a list of package names.\n",
    "    If version is available, returns 'package==version'.\n",
    "    If not installed or no version, returns just 'package'.\n",
    "    \"\"\"\n",
    "    import importlib\n",
    "\n",
    "    packages = {\n",
    "        \"nibabel\": \"nibabel\",\n",
    "        \"numpy\": \"numpy\",\n",
    "        \"pandas\": \"pandas\",\n",
    "        \"scipy\": \"scipy\",\n",
    "        \"plotly\": \"plotly\",\n",
    "        \"scikit-image\": \"skimage\",\n",
    "        \"kaleido\": \"kaleido\"\n",
    "    }\n",
    "\n",
    "    result = []\n",
    "    for pip_name, module_name in packages.items():\n",
    "        try:\n",
    "            mod = importlib.import_module(module_name)\n",
    "            version = getattr(mod, \"__version__\", None)\n",
    "            if version is not None:\n",
    "                result.append(f\"{pip_name}=={version}\")\n",
    "            else:\n",
    "                result.append(pip_name)\n",
    "        except ImportError:\n",
    "            result.append(pip_name)  # not installed ‚Üí show name only\n",
    "    return result\n",
    "\n",
    "\n",
    "def install_missing_packages(requirements_list):\n",
    "    \"\"\"\n",
    "    Install packages from a list of strings like ['numpy==1.26', 'pandas==2.2'].\n",
    "    Only installs if the package is NOT already installed (any version).\n",
    "    \n",
    "    Parameters:\n",
    "    requirements_list (list of str): e.g. ['nibabel==5.2.1', 'kaleido==0.2.1']\n",
    "    \"\"\"\n",
    "    to_install = []\n",
    "    \n",
    "    for req in requirements_list:\n",
    "        # ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ ŸÜÿßŸÖ Ÿæ⁄©€åÿ¨ ÿßÿ≤ 'nibabel==5.2.1' ‚Üí 'nibabel'\n",
    "        package_name = req.split(\"==\")[0].split(\">=\")[0].split(\"<=\")[0].split(\"!=\")[0]\n",
    "        \n",
    "        # ŸÜÿßŸÖ ŸÖÿß⁄òŸàŸÑ ŸÖÿ±ÿ®Ÿàÿ∑Ÿá (scikit-image ‚Üí skimage)\n",
    "        module_name = \"skimage\" if package_name == \"scikit-image\" else package_name\n",
    "        \n",
    "        try:\n",
    "            importlib.import_module(module_name)\n",
    "        except ImportError:\n",
    "            to_install.append(req)\n",
    "    \n",
    "    if to_install:\n",
    "        print(f\"Installing missing packages: {to_install}\")\n",
    "        subprocess.check_call(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\"] + to_install\n",
    "        )\n",
    "    else:\n",
    "        print(\"All packages already installed.\")\n",
    "\n",
    "install_missing_packages(get_installed_versions())  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3238b8e6-ae2b-447e-8b24-ce4b358fb047",
   "metadata": {},
   "source": [
    "### üî¨ Step 1: Functional Parcellation and Representative Time Series Extraction\n",
    "\n",
    "In this step, two regions of interest ‚Äî **left DLPFC** and **bilateral sgACC** ‚Äî are functionally parcellated **independently**, using a **hierarchical clustering approach based on inter-voxel time series similarity**.\n",
    "\n",
    "#### üìå Key Stages:\n",
    "\n",
    "**1. Time Series Extraction:**  \n",
    "Voxel-wise BOLD time series are extracted from the preprocessed fMRI data within each ROI mask.\n",
    "\n",
    "**2. Pairwise Similarity Computation:**  \n",
    "A correlation matrix is computed across all voxels in each ROI using one of the following methods:\n",
    "- **Spearman rank correlation** (default, robust to outliers)  \n",
    "- **Pearson correlation** (linear dependence)  \n",
    "- **Kendall‚Äôs tau** (rank-based, more robust but computationally intensive)  \n",
    "- **Zero-lag cross-correlation** (Pearson on z-scored time series)\n",
    "\n",
    "**3. Hierarchical Agglomerative Clustering (HAC):**  \n",
    "- Distance is defined as `distance = 1 - œÅ` (where œÅ is the correlation coefficient).  \n",
    "- Clustering uses **average linkage**.  \n",
    "- The dendrogram is cut at a **distance threshold ‚â§ 0.5**, equivalent to requiring **pairwise correlation œÅ ‚â• 0.5** within each subunit.\n",
    "\n",
    "**4. Representative Voxel Selection:**  \n",
    "For each functional subunit:  \n",
    "- The **median time series** of all voxels in the cluster is computed.  \n",
    "- The **representative voxel** is selected as the one whose time series has the **highest correlation** (using the same method as step 2) with this median.\n",
    "\n",
    "**5. Output Generation:**  \n",
    "- A **summary Excel file** (`subXXX_SAINT_Protocol_Output_{method}.xlsx`) containing:  \n",
    "  - Cluster IDs, sizes, representative voxel coordinates, and full time series.  \n",
    "- A **compressed `.npz` file** (`subXXX_cluster_voxel_coords_{method}.npz`) storing:  \n",
    "  - Full voxel coordinates for every cluster.  \n",
    "  - Cluster labels and the correlation method used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b3b6a1-4386-4aac-9139-ac7128e21ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final pipeline: Spearman-based HAC (cut at rho >= 0.5), representative voxel selection\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from scipy.stats import spearmanr, pearsonr, kendalltau\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# -------------------------\n",
    "# CONFIGURATION\n",
    "# -------------------------\n",
    "rho_threshold = 0.5\n",
    "base_dir = \"G:/SAINT/Subjects\"\n",
    "# -------------------------\n",
    "# Helper: find mask file (prefer *_bin, fallback to *_func)\n",
    "# -------------------------\n",
    "def find_mask_file(subject_dir, base_name):\n",
    "    \"\"\"Try bin first, then func.\"\"\"\n",
    "    bin_path = os.path.join(subject_dir, f\"{base_name}_bin.nii.gz\")\n",
    "    func_path = os.path.join(subject_dir, f\"{base_name}_func.nii.gz\")\n",
    "    if os.path.exists(func_path):\n",
    "        return func_path\n",
    "    elif os.path.exists(bin_path):\n",
    "        return bin_path\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# -------------------------\n",
    "# Correlation matrix computation with multiple methods\n",
    "# -------------------------\n",
    "def compute_correlation_matrix(voxel_ts, method=\"spearman\"):\n",
    "    \"\"\"\n",
    "    Compute correlation matrix using specified method.\n",
    "    Supported methods: 'spearman', 'pearson', 'kendall', 'crosscorr'\n",
    "    \"\"\"\n",
    "    if voxel_ts.size == 0:\n",
    "        return np.array([[]])\n",
    "    \n",
    "    V, T = voxel_ts.shape\n",
    "    \n",
    "    # Handle single-voxel case: correlation matrix is [[1.0]]\n",
    "    if V == 1:\n",
    "        return np.array([[1.0]])\n",
    "    \n",
    "    if method == \"spearman\":\n",
    "        rho, _ = spearmanr(voxel_ts, axis=1)\n",
    "    elif method == \"pearson\":\n",
    "        # ‚úÖ Use np.corrcoef ‚Äî pearsonr does NOT support axis!\n",
    "        rho = np.corrcoef(voxel_ts)\n",
    "    elif method == \"crosscorr\":\n",
    "        # Zero-lag cross-correlation = Pearson on z-scored time series\n",
    "        ts_norm = (voxel_ts - voxel_ts.mean(axis=1, keepdims=True)) / (voxel_ts.std(axis=1, keepdims=True) + 1e-10)\n",
    "        rho = np.corrcoef(ts_norm)\n",
    "    elif method == \"kendall\":\n",
    "        # Warning for large ROIs\n",
    "        if V > 300:\n",
    "            print(f\"‚ö†Ô∏è Warning: Kendall‚Äôs tau is very slow for large ROIs (V={V}). Consider using spearman/pearson.\")\n",
    "        rho = np.eye(V)\n",
    "        for i in range(V):\n",
    "            for j in range(i + 1, V):\n",
    "                tau, _ = kendalltau(voxel_ts[i], voxel_ts[j])\n",
    "                rho[i, j] = rho[j, i] = tau if not np.isnan(tau) else 0.0\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported correlation method: {method}. Choose from: spearman, pearson, kendall, crosscorr\")\n",
    "    \n",
    "    rho = np.nan_to_num(rho, nan=0.0)\n",
    "    return np.clip(rho, -1.0, 1.0)\n",
    "\n",
    "# -------------------------\n",
    "# Main processing function for ONE subject\n",
    "# -------------------------\n",
    "def process_subject(subject_id, base_dir=\"G:/SAINT/Subjects\", corr_method=\"spearman\"):\n",
    "    subject_dir = os.path.join(base_dir, f\"sub{subject_id}\")\n",
    "    fmri_path = os.path.join(subject_dir, \"filtered_func_data.nii.gz\")\n",
    "    \n",
    "    # Validate fMRI file\n",
    "    if not os.path.exists(fmri_path):\n",
    "        print(f\"‚ö†Ô∏è Skipping sub{subject_id}: fMRI file not found.\")\n",
    "        return False\n",
    "\n",
    "    # Find mask files\n",
    "    dlpfc_mask_path = find_mask_file(subject_dir, \"l_DLPFC\")\n",
    "    sgacc_mask_path = find_mask_file(subject_dir, \"sgACC\")\n",
    "    \n",
    "    if dlpfc_mask_path is None:\n",
    "        print(f\"‚ö†Ô∏è Skipping sub{subject_id}: DLPFC mask not found (needs l_DLPFC_bin.nii.gz or l_DLPFC_func.nii.gz).\")\n",
    "        return False\n",
    "    if sgacc_mask_path is None:\n",
    "        print(f\"‚ö†Ô∏è Skipping sub{subject_id}: sgACC mask not found (needs sgACC_bin.nii.gz or sgACC_func.nii.gz).\")\n",
    "        return False\n",
    "\n",
    "    print(f\"\\nüîç Processing sub{subject_id} with correlation method: {corr_method}...\")\n",
    "    \n",
    "    # Load data\n",
    "    fmri_img = nib.load(fmri_path)\n",
    "    fmri_data = fmri_img.get_fdata()\n",
    "    affine = fmri_img.affine\n",
    "\n",
    "    dlpfc_mask = nib.load(dlpfc_mask_path).get_fdata().astype(bool)\n",
    "    sgacc_mask = nib.load(sgacc_mask_path).get_fdata().astype(bool)\n",
    "    brain_mask = np.any(fmri_data != 0, axis=3)\n",
    "\n",
    "    print(\"Voxels: brain={}, DLPFC={}, sgACC={}\".format(\n",
    "        np.sum(brain_mask), np.sum(dlpfc_mask), np.sum(sgacc_mask)\n",
    "    ))\n",
    "\n",
    "    # Extract time series\n",
    "    def extract_mask_timeseries(fmri_data, mask):\n",
    "        inds = np.argwhere(mask)\n",
    "        ts = np.array([fmri_data[x, y, z, :] for x, y, z in inds])\n",
    "        return inds, ts\n",
    "\n",
    "    dlpfc_inds, dlpfc_ts = extract_mask_timeseries(fmri_data, dlpfc_mask)\n",
    "    sgacc_inds, sgacc_ts = extract_mask_timeseries(fmri_data, sgacc_mask)\n",
    "\n",
    "    print(\"DLPFC time series shape (V, T):\", dlpfc_ts.shape)\n",
    "    print(\"sgACC time series shape (V, T):\", sgacc_ts.shape)\n",
    "\n",
    "    # Spearman correlation ‚Üí replaced with general correlation\n",
    "    rho_dlpfc = compute_correlation_matrix(dlpfc_ts, method=corr_method)\n",
    "    rho_sgacc = compute_correlation_matrix(sgacc_ts, method=corr_method)\n",
    "\n",
    "    # Distance matrix\n",
    "    dist_dlpfc = 1.0 - rho_dlpfc\n",
    "    dist_sgacc = 1.0 - rho_sgacc\n",
    "    np.fill_diagonal(dist_dlpfc, 0.0)\n",
    "    np.fill_diagonal(dist_sgacc, 0.0)\n",
    "\n",
    "    # HAC clustering\n",
    "    def hac_cut_by_rho(dist_matrix, rho_threshold=0.5, method='average'):\n",
    "        if dist_matrix.size == 0:\n",
    "            return np.array([], dtype=int)\n",
    "        condensed = squareform(dist_matrix, checks=False)\n",
    "        Z = linkage(condensed, method=method)\n",
    "        max_dist = 1.0 - rho_threshold\n",
    "        return fcluster(Z, t=max_dist, criterion='distance').astype(int)\n",
    "\n",
    "    dlpfc_labels = hac_cut_by_rho(dist_dlpfc, rho_threshold=rho_threshold)\n",
    "    sgacc_labels = hac_cut_by_rho(dist_sgacc, rho_threshold=rho_threshold)\n",
    "\n",
    "    print(\"Number of DLPFC clusters:\", len(np.unique(dlpfc_labels)))\n",
    "    print(\"Number of sgACC clusters :\", len(np.unique(sgacc_labels)))\n",
    "\n",
    "    # Representative selection + keep all voxel coords\n",
    "    def representatives_from_clusters(inds, voxel_ts, labels):\n",
    "        reps = []\n",
    "        for lab in np.unique(labels):\n",
    "            mask = (labels == lab)\n",
    "            cluster_ts = voxel_ts[mask]\n",
    "            cluster_coords = inds[mask]  # ‚Üê ALL voxels in this cluster\n",
    "            if cluster_ts.shape[0] == 0:\n",
    "                continue\n",
    "            median_ts = np.median(cluster_ts, axis=0)\n",
    "            if cluster_ts.shape[0] == 1:\n",
    "                chosen_voxel = cluster_coords[0]\n",
    "                chosen_ts = cluster_ts[0]\n",
    "            else:\n",
    "                stack = np.vstack([median_ts, cluster_ts])\n",
    "                # Use the same correlation method for representative selection\n",
    "                corr_mat = compute_correlation_matrix(stack, method=corr_method)\n",
    "                rho_with_median = corr_mat[0, 1:]\n",
    "                chosen_idx = int(np.argmax(rho_with_median))\n",
    "                chosen_voxel = cluster_coords[chosen_idx]\n",
    "                chosen_ts = cluster_ts[chosen_idx]\n",
    "            reps.append({\n",
    "                \"cluster_label\": int(lab),\n",
    "                \"voxel_coords\": cluster_coords,          # ‚Üê FULL list of voxel coordinates (Nx3)\n",
    "                \"rep_coords\": tuple(map(int, chosen_voxel)),\n",
    "                \"timeseries\": chosen_ts,\n",
    "                \"size\": cluster_ts.shape[0]\n",
    "            })\n",
    "        return reps\n",
    "\n",
    "    dlpfc_reps = representatives_from_clusters(dlpfc_inds, dlpfc_ts, dlpfc_labels)\n",
    "    sgacc_reps = representatives_from_clusters(sgacc_inds, sgacc_ts, sgacc_labels)\n",
    "\n",
    "    print(\"DLPFC representatives:\", len(dlpfc_reps))\n",
    "    print(\"sgACC representatives :\", len(sgacc_reps))\n",
    "\n",
    "    # -------------------------\n",
    "    # SAVE 1: Excel summary (filename includes corr_method)\n",
    "    # -------------------------\n",
    "    output_dir = os.path.join(subject_dir, \"SAINT-PROTOCOL\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = os.path.join(output_dir, f\"sub{subject_id}_SAINT_Protocol_Output_{corr_method}.xlsx\")\n",
    "\n",
    "    def build_cluster_df(reps):\n",
    "        if not reps:\n",
    "            return pd.DataFrame()\n",
    "        df = pd.DataFrame([{\n",
    "            \"ClusterID\": r[\"cluster_label\"],\n",
    "            \"Size\": r[\"size\"],\n",
    "            \"RepVoxel_X\": r[\"rep_coords\"][0],\n",
    "            \"RepVoxel_Y\": r[\"rep_coords\"][1],\n",
    "            \"RepVoxel_Z\": r[\"rep_coords\"][2]\n",
    "        } for r in reps])\n",
    "        ts_array = np.array([r[\"timeseries\"] for r in reps])\n",
    "        ts_df = pd.DataFrame(ts_array, columns=[f\"T{i}\" for i in range(ts_array.shape[1])])\n",
    "        return pd.concat([df, ts_df], axis=1)\n",
    "\n",
    "    dlpfc_df = build_cluster_df(dlpfc_reps)\n",
    "    sgacc_df = build_cluster_df(sgacc_reps)\n",
    "\n",
    "    summary_df = pd.DataFrame({\n",
    "        \"Region\": [\"DLPFC\", \"sgACC\"],\n",
    "        \"N_Clusters\": [len(dlpfc_reps), len(sgacc_reps)],\n",
    "        \"Correlation_Method\": [corr_method, corr_method]\n",
    "    })\n",
    "\n",
    "    with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
    "        summary_df.to_excel(writer, sheet_name='Summary', index=False)\n",
    "        if not dlpfc_df.empty:\n",
    "            dlpfc_df.to_excel(writer, sheet_name='DLPFC_Clusters', index=False)\n",
    "        if not sgacc_df.empty:\n",
    "            sgacc_df.to_excel(writer, sheet_name='sgACC_Clusters', index=False)\n",
    "\n",
    "    # -------------------------\n",
    "    # SAVE 2: Full voxel coordinates in .npz file (filename includes corr_method)\n",
    "    # -------------------------\n",
    "    voxel_coords_path = os.path.join(output_dir, f\"sub{subject_id}_cluster_voxel_coords_{corr_method}.npz\")\n",
    "    \n",
    "    # Convert lists to object arrays for safe saving\n",
    "    dlpfc_voxel_arrays = [r[\"voxel_coords\"] for r in dlpfc_reps]\n",
    "    sgacc_voxel_arrays = [r[\"voxel_coords\"] for r in sgacc_reps]\n",
    "    \n",
    "    np.savez_compressed(\n",
    "        voxel_coords_path,\n",
    "        dlpfc_voxel_coords=np.array(dlpfc_voxel_arrays, dtype=object),\n",
    "        sgacc_voxel_coords=np.array(sgacc_voxel_arrays, dtype=object),\n",
    "        dlpfc_cluster_labels=np.array([r[\"cluster_label\"] for r in dlpfc_reps]),\n",
    "        sgacc_cluster_labels=np.array([r[\"cluster_label\"] for r in sgacc_reps]),\n",
    "        correlation_method=corr_method,\n",
    "        allow_pickle=True\n",
    "    )\n",
    "\n",
    "    print(f\"\\n‚úÖ Results ({corr_method}) saved to:\\n{output_path}\")\n",
    "    print(f\"üì¶ Full voxel coordinates ({corr_method}) saved to:\\n{voxel_coords_path}\")\n",
    "    print(f\"- DLPFC clusters: {len(dlpfc_reps)}\")\n",
    "    print(f\"- sgACC clusters: {len(sgacc_reps)}\")\n",
    "    return True\n",
    "\n",
    "# -------------------------\n",
    "# -------------------------\n",
    "# Main execution logic\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # ‚öôÔ∏è CONFIGURATION\n",
    "    \n",
    "    # Choose mode:\n",
    "    run_all_methods = True  # Set to False to run only one method\n",
    "    corr_method = \"spearman\"  # Used only if run_all_methods = False\n",
    "\n",
    "    # List of all supported correlation methods\n",
    "    all_methods = [\"spearman\", \"pearson\", \"kendall\", \"crosscorr\"]\n",
    "\n",
    "    # Decide which methods to run\n",
    "    methods_to_run = all_methods if run_all_methods else [corr_method]\n",
    "\n",
    "    # Single subject or batch?\n",
    "    subject_id = None  # Set to e.g., \"001\" for single subject; None for batch\n",
    "\n",
    "    if subject_id is not None:\n",
    "        # Single subject: run all selected methods\n",
    "        for method in methods_to_run:\n",
    "            print(f\"\\nüöÄ Running Algorithm 1 for sub{subject_id} with method: {method}\")\n",
    "            success = process_subject(subject_id, base_dir, corr_method=method)\n",
    "            if success:\n",
    "                print(f\"\\nüéâ Subject sub{subject_id} processed successfully with {method}!\")\n",
    "            else:\n",
    "                print(f\"\\n‚ùå Failed to process sub{subject_id} with {method}.\")\n",
    "    else:\n",
    "        # Batch mode: for each subject, run all selected methods\n",
    "        subject_folders = [f for f in os.listdir(base_dir) if re.match(r'^sub\\d+$', f)]\n",
    "        subject_ids = sorted([f.replace('sub', '') for f in subject_folders], key=int)\n",
    "        total_subjects = len(subject_ids)\n",
    "\n",
    "        print(f\"Found {total_subjects} subjects. Starting batch processing...\\n\")\n",
    "        \n",
    "        for sid in subject_ids:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Processing subject: sub{sid}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            completed_methods = 0\n",
    "            for method in methods_to_run:\n",
    "                try:\n",
    "                    print(f\"\\n‚Üí Running method: {method}\")\n",
    "                    success = process_subject(sid, base_dir, corr_method=method)\n",
    "                    if success:\n",
    "                        completed_methods += 1\n",
    "                        print(f\"‚úÖ Completed {method} for sub{sid}\")\n",
    "                    else:\n",
    "                        print(f\"‚ö†Ô∏è Skipped {method} for sub{sid} (missing files)\")\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Error in {method} for sub{sid}: {e}\")\n",
    "            \n",
    "            print(f\"\\nüìå Summary for sub{sid}: {completed_methods}/{len(methods_to_run)} methods completed.\")\n",
    "\n",
    "        print(f\"\\nüéâ Batch processing complete for {total_subjects} subjects!\")\n",
    "        print(f\"Methods run: {', '.join(methods_to_run)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eada138-4976-4036-a096-04106af954b8",
   "metadata": {},
   "source": [
    "### üîç Step 2: Optimal DLPFC Subunit Selection\n",
    "\n",
    "Building on the functional parcellation from **Algorithm 1**, this step identifies the **single optimal DLPFC subunit** for targeting, based on three biologically and clinically motivated criteria.\n",
    "\n",
    "#### ‚ö†Ô∏è Prerequisite:\n",
    "> **Algorithm 1 must be successfully executed first** for the same subject(s) and with the **same correlation method** (e.g., `spearman`, `pearson`, `kendall`, or `crosscorr`).  \n",
    "> This step **depends entirely** on two output files generated by Algorithm 1:\n",
    "> - `subXXX_SAINT_Protocol_Output_{method}.xlsx` (containing DLPFC/sgACC cluster time series)\n",
    "> - `subXXX_cluster_voxel_coords_{method}.npz` (containing full voxel coordinates for spatial concentration)\n",
    "\n",
    "If these files are missing or inconsistent (e.g., different correlation method), Algorithm 2 will **skip** the subject.\n",
    "\n",
    "#### üìå Key Components:\n",
    "\n",
    "**1. Net Functional Coupling with sgACC:**  \n",
    "- For each DLPFC subunit, compute the **weighted net correlation** with all sgACC subunits:  \n",
    "  `NetCorrelation = Œ£ (œÅ_DLPFC‚ÄìsgACC √ó sgACC_subunit_size)`  \n",
    "- **Stronger anticorrelation (more negative net correlation) is preferred**, as it aligns with the therapeutic hypothesis of the SAINT protocol.\n",
    "\n",
    "**2. Spatial Concentration:**  \n",
    "- Measures how compact a subunit is in 3D space:  \n",
    "  `SpatialConcentration = (Number of voxels) / (Mean pairwise Euclidean distance)`  \n",
    "- **Higher values indicate tighter, more focal clusters**, which are preferable for precise neuromodulation.\n",
    "\n",
    "**3. Cluster Size:**  \n",
    "- Larger subunits are easier to target reliably with TMS coils.  \n",
    "- Size is included as a direct feature in the scoring.\n",
    "\n",
    "**4. Composite Scoring & Selection:**  \n",
    "- Each metric is **min-max normalized** to [0, 1] (optional via `USE_NORMALIZATION`).  \n",
    "- A weighted final score is computed:  \n",
    "  `FinalScore = Œ±¬∑AnticorrScore + Œ≤¬∑SpatialScore + Œ≥¬∑SizeScore`  \n",
    "  where:  \n",
    "  - `Œ± = 0.5` (anticorrelation strength)  \n",
    "  - `Œ≤ = 0.35` (spatial concentration)  \n",
    "  - `Œ≥ = 0.15` (cluster size)  \n",
    "- The DLPFC subunit with the **highest FinalScore** is selected as optimal.\n",
    "\n",
    "**5. Output:**  \n",
    "- Results are saved in a new sheet **`Optimal_DLPFC`** within the same Excel file from Algorithm 1 (`subXXX_SAINT_Protocol_Output_{method}.xlsx`).  \n",
    "- The output includes: cluster ID, centroid coordinates, raw metrics, normalized scores, weights, final score, and the correlation method used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928844cd-6829-4b30-8988-22cbc1b24035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# Algorithm 2: Optimal DLPFC Subunit Selection\n",
    "# Uses full voxel coordinates from .npz file (saved by Algorithm 1)\n",
    "# Supports multiple correlation methods: 'spearman', 'pearson', 'kendall', 'crosscorr'\n",
    "# Processes one subject or all subjects in batch\n",
    "# =========================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from scipy.stats import spearmanr, pearsonr, kendalltau\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "# -------------------------\n",
    "# CONFIGURATION\n",
    "# -------------------------\n",
    "USE_NORMALIZATION = True\n",
    "alpha = 0.5   # weight for anticorrelation strength\n",
    "beta = 0.35   # weight for spatial concentration\n",
    "gamma = 0.15  # weight for cluster size\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Helper: compute correlation based on method\n",
    "# -------------------------\n",
    "# -------------------------\n",
    "# Helper: compute correlation based on method\n",
    "# -------------------------\n",
    "def compute_correlation(ts1, ts2, method=\"spearman\"):\n",
    "    \"\"\"Compute correlation between two time series with given method.\"\"\"\n",
    "    # Ensure both time series have at least 2 time points\n",
    "    if len(ts1) < 2 or len(ts2) < 2:\n",
    "        return 0.0\n",
    "    \n",
    "    try:\n",
    "        if method == \"spearman\":\n",
    "            rho, _ = spearmanr(ts1, ts2)\n",
    "        elif method == \"pearson\":\n",
    "            rho, _ = pearsonr(ts1, ts2)\n",
    "        elif method == \"kendall\":\n",
    "            rho, _ = kendalltau(ts1, ts2)\n",
    "        elif method == \"crosscorr\":\n",
    "            def zscore(x):\n",
    "                return (x - x.mean()) / (x.std() + 1e-8)\n",
    "            ts1_z = zscore(ts1)\n",
    "            ts2_z = zscore(ts2)\n",
    "            rho = np.corrcoef(ts1_z, ts2_z)[0, 1]\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported correlation method: {method}\")\n",
    "        \n",
    "        return 0.0 if np.isnan(rho) else float(rho)\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "# -------------------------\n",
    "# Helper functions\n",
    "# -------------------------\n",
    "def spatial_concentration(coords):\n",
    "    \"\"\"Compute spatial concentration: N_voxels / mean pairwise distance.\"\"\"\n",
    "    if coords.shape[0] < 2:\n",
    "        return 0.0\n",
    "    distances = pdist(coords, metric=\"euclidean\")\n",
    "    return coords.shape[0] / np.mean(distances)\n",
    "\n",
    "def min_max_norm(x):\n",
    "    \"\"\"Min-Max normalize to [0, 1].\"\"\"\n",
    "    x = np.array(x, dtype=float)\n",
    "    if x.size == 0 or x.max() == x.min():\n",
    "        return np.ones_like(x)\n",
    "    return (x - x.min()) / (x.max() - x.min())\n",
    "\n",
    "# -------------------------\n",
    "# Main processing function for ONE subject\n",
    "# -------------------------\n",
    "def process_subject_algo2(subject_id, base_dir=\"G:/SAINT/Subjects\", corr_method=\"spearman\"):\n",
    "    \"\"\"Run Algorithm 2 for a single subject using full voxel data from Algorithm 1.\"\"\"\n",
    "    print(f\"\\nüîç Running Algorithm 2 for sub{subject_id} (method: {corr_method})...\")\n",
    "    \n",
    "    output_dir = os.path.join(base_dir, f\"sub{subject_id}\", \"SAINT-PROTOCOL\")\n",
    "    # Dynamically build input file names based on corr_method\n",
    "    output_path = os.path.join(output_dir, f\"sub{subject_id}_SAINT_Protocol_Output_{corr_method}.xlsx\")\n",
    "    voxel_coords_path = os.path.join(output_dir, f\"sub{subject_id}_cluster_voxel_coords_{corr_method}.npz\")\n",
    "    \n",
    "    # Check if required files exist\n",
    "    if not os.path.exists(output_path):\n",
    "        print(f\"‚ö†Ô∏è Skipping sub{subject_id}: Algorithm 1 Excel output not found for method '{corr_method}'.\")\n",
    "        return False\n",
    "    if not os.path.exists(voxel_coords_path):\n",
    "        print(f\"‚ö†Ô∏è Skipping sub{subject_id}: Full voxel coordinates (.npz) not found for method '{corr_method}'.\")\n",
    "        return False\n",
    "\n",
    "    # Load cluster data from Excel\n",
    "    try:\n",
    "        dlpfc_df = pd.read_excel(output_path, sheet_name='DLPFC_Clusters')\n",
    "        sgacc_df = pd.read_excel(output_path, sheet_name='sgACC_Clusters')\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Skipping sub{subject_id}: Error reading Excel: {e}\")\n",
    "        return False\n",
    "\n",
    "    if dlpfc_df.empty:\n",
    "        print(f\"‚ö†Ô∏è Skipping sub{subject_id}: No DLPFC clusters found.\")\n",
    "        return False\n",
    "    if sgacc_df.empty:\n",
    "        print(f\"‚ö†Ô∏è Skipping sub{subject_id}: No sgACC clusters found.\")\n",
    "        return False\n",
    "\n",
    "    # Load full voxel coordinates from .npz\n",
    "    try:\n",
    "        voxel_data = np.load(voxel_coords_path, allow_pickle=True)\n",
    "        dlpfc_voxel_arrays = voxel_data['dlpfc_voxel_coords']\n",
    "        sgacc_voxel_arrays = voxel_data['sgacc_voxel_coords']\n",
    "        dlpfc_labels_np = voxel_data['dlpfc_cluster_labels']\n",
    "        sgacc_labels_np = voxel_data['sgacc_cluster_labels']\n",
    "        saved_method = voxel_data.get('correlation_method', corr_method)\n",
    "        if saved_method != corr_method:\n",
    "            print(f\"‚ö†Ô∏è Warning: .npz file was saved with method '{saved_method}', but you're using '{corr_method}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Skipping sub{subject_id}: Error loading .npz file: {e}\")\n",
    "        return False\n",
    "\n",
    "    # Reconstruct dlpfc_reps and sgacc_reps with FULL voxel_coords\n",
    "    def df_and_voxels_to_reps(df, voxel_arrays, labels_array):\n",
    "        reps = []\n",
    "        label_to_voxels = dict(zip(labels_array, voxel_arrays))\n",
    "        ts_cols = [col for col in df.columns if col.startswith('T')]\n",
    "        for _, row in df.iterrows():\n",
    "            cluster_id = int(row[\"ClusterID\"])\n",
    "            ts = df.loc[df.index == row.name, ts_cols].values.flatten()\n",
    "            voxel_coords = label_to_voxels.get(cluster_id)\n",
    "            if voxel_coords is None:\n",
    "                print(f\"‚ö†Ô∏è Warning: Cluster {cluster_id} not found in .npz file.\")\n",
    "                continue\n",
    "            reps.append({\n",
    "                \"cluster_label\": cluster_id,\n",
    "                \"rep_coords\": (int(row[\"RepVoxel_X\"]), int(row[\"RepVoxel_Y\"]), int(row[\"RepVoxel_Z\"])),\n",
    "                \"timeseries\": ts,\n",
    "                \"size\": int(row[\"Size\"]),\n",
    "                \"voxel_coords\": voxel_coords\n",
    "            })\n",
    "        return reps\n",
    "\n",
    "    dlpfc_reps = df_and_voxels_to_reps(dlpfc_df, dlpfc_voxel_arrays, dlpfc_labels_np)\n",
    "    sgacc_reps = df_and_voxels_to_reps(sgacc_df, sgacc_voxel_arrays, sgacc_labels_np)\n",
    "\n",
    "    if not dlpfc_reps:\n",
    "        print(f\"‚ö†Ô∏è Skipping sub{subject_id}: No matching DLPFC clusters in .npz.\")\n",
    "        return False\n",
    "\n",
    "    # Step 1: Compute metrics using FULL voxel_coords\n",
    "    raw_net_corrs = []\n",
    "    raw_sizes = []\n",
    "    raw_spatial_concs = []\n",
    "    cluster_labels = []\n",
    "    centroids = []\n",
    "\n",
    "    for d in dlpfc_reps:\n",
    "        net_corr = 0.0\n",
    "        for s in sgacc_reps:\n",
    "            # ‚úÖ Use the specified correlation method!\n",
    "            rho = compute_correlation(d[\"timeseries\"], s[\"timeseries\"], method=corr_method)\n",
    "            net_corr += rho * s[\"size\"]\n",
    "\n",
    "        size = d[\"size\"]\n",
    "        spatial = spatial_concentration(d[\"voxel_coords\"])\n",
    "        centroid = d[\"rep_coords\"]\n",
    "\n",
    "        raw_net_corrs.append(net_corr)\n",
    "        raw_sizes.append(size)\n",
    "        raw_spatial_concs.append(spatial)\n",
    "        cluster_labels.append(d[\"cluster_label\"])\n",
    "        centroids.append(centroid)\n",
    "\n",
    "    # Step 2: Normalize\n",
    "    if USE_NORMALIZATION:\n",
    "        anticorr_scores = min_max_norm([-c for c in raw_net_corrs])\n",
    "        spatial_scores = min_max_norm(raw_spatial_concs)\n",
    "        size_scores = min_max_norm(raw_sizes)\n",
    "    else:\n",
    "        anticorr_scores = np.array([-c for c in raw_net_corrs])\n",
    "        spatial_scores = np.array(raw_spatial_concs)\n",
    "        size_scores = np.array(raw_sizes)\n",
    "\n",
    "    # Step 3: Final score\n",
    "    final_scores = (\n",
    "        alpha * anticorr_scores +\n",
    "        beta * spatial_scores +\n",
    "        gamma * size_scores\n",
    "    )\n",
    "\n",
    "    # Step 4: Prepare results\n",
    "    results = []\n",
    "    for i, label in enumerate(cluster_labels):\n",
    "        results.append({\n",
    "            \"Subject\": f\"sub{subject_id}\",\n",
    "            \"DLPFC_Cluster\": label,\n",
    "            \"Centroid_X\": float(centroids[i][0]),\n",
    "            \"Centroid_Y\": float(centroids[i][1]),\n",
    "            \"Centroid_Z\": float(centroids[i][2]),\n",
    "            \"NetCorrelation\": raw_net_corrs[i],\n",
    "            \"ClusterSize\": raw_sizes[i],\n",
    "            \"SpatialConcentration\": raw_spatial_concs[i],\n",
    "            \"AnticorrScore\": anticorr_scores[i],\n",
    "            \"SpatialScore\": spatial_scores[i],\n",
    "            \"SizeScore\": size_scores[i],\n",
    "            \"Alpha\": alpha,\n",
    "            \"Beta\": beta,\n",
    "            \"Gamma\": gamma,\n",
    "            \"FinalScore\": final_scores[i],\n",
    "            \"Normalized\": USE_NORMALIZATION,\n",
    "            \"CorrelationMethod\": corr_method\n",
    "        })\n",
    "\n",
    "    df_algo2 = pd.DataFrame(results)\n",
    "    df_algo2 = df_algo2.sort_values(\"FinalScore\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # Step 5: Update Excel file (same file as input)\n",
    "    with pd.ExcelFile(output_path, engine='openpyxl') as reader:\n",
    "        existing_sheets = {sheet: reader.parse(sheet) for sheet in reader.sheet_names}\n",
    "\n",
    "    existing_sheets['Optimal_DLPFC'] = df_algo2\n",
    "\n",
    "    with pd.ExcelWriter(output_path, engine='openpyxl', mode='w') as writer:\n",
    "        for sheet_name, sheet_df in existing_sheets.items():\n",
    "            sheet_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "    # Step 6: Print results\n",
    "    best = df_algo2.iloc[0]\n",
    "    print(f\"‚úÖ Best DLPFC Subunit: Cluster {best['DLPFC_Cluster']}\")\n",
    "    print(f\"   Centroid: ({best['Centroid_X']:.1f}, {best['Centroid_Y']:.1f}, {best['Centroid_Z']:.1f})\")\n",
    "    print(f\"   NetCorrelation: {best['NetCorrelation']:.3f}\")\n",
    "    print(f\"   SpatialConcentration: {best['SpatialConcentration']:.3f}\")\n",
    "    print(f\"   FinalScore: {best['FinalScore']:.3f}\")\n",
    "    print(f\"\\nTop 3 DLPFC subunits:\")\n",
    "    print(df_algo2.head(3)[[\"DLPFC_Cluster\", \"Centroid_X\", \"Centroid_Y\", \"Centroid_Z\", \"NetCorrelation\", \"SpatialConcentration\", \"FinalScore\"]])\n",
    "    print(f\"\\nüìä Full results saved to sheet 'Optimal_DLPFC' in:\\n{output_path}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# -------------------------\n",
    "# -------------------------\n",
    "# Main execution logic\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # ‚öôÔ∏è CONFIGURATION\n",
    "    \n",
    "    # Choose mode:\n",
    "    run_all_methods = True  # Set to False to run only one method\n",
    "    corr_method = \"spearman\"  # Used only if run_all_methods = False\n",
    "\n",
    "    # List of all supported correlation methods\n",
    "    all_methods = [\"spearman\", \"pearson\", \"kendall\", \"crosscorr\"]\n",
    "\n",
    "    # Decide which methods to run\n",
    "    methods_to_run = all_methods if run_all_methods else [corr_method]\n",
    "\n",
    "    # Single subject or batch?\n",
    "    subject_id = None  # Set to e.g., \"001\" for single subject; None for batch\n",
    "\n",
    "    if subject_id is not None:\n",
    "        # Single subject: run all selected methods\n",
    "        for method in methods_to_run:\n",
    "            print(f\"\\nüöÄ Running Algorithm 2 for sub{subject_id} with method: {method}\")\n",
    "            success = process_subject_algo2(subject_id, base_dir, corr_method=method)\n",
    "            if success:\n",
    "                print(f\"\\nüéâ Algorithm 2 completed successfully for sub{subject_id} ({method})!\")\n",
    "            else:\n",
    "                print(f\"\\n‚ùå Algorithm 2 failed for sub{subject_id} with {method}.\")\n",
    "    else:\n",
    "        # Batch mode: for each subject, run all selected methods\n",
    "        subject_folders = [f for f in os.listdir(base_dir) if re.match(r'^sub\\d+$', f)]\n",
    "        subject_ids = sorted([f.replace('sub', '') for f in subject_folders], key=int)\n",
    "        total_subjects = len(subject_ids)\n",
    "\n",
    "        print(f\"Found {total_subjects} subjects. Starting Algorithm 2 batch processing...\\n\")\n",
    "        \n",
    "        for sid in subject_ids:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Processing subject: sub{sid}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            completed_methods = 0\n",
    "            for method in methods_to_run:\n",
    "                try:\n",
    "                    print(f\"\\n‚Üí Running method: {method}\")\n",
    "                    success = process_subject_algo2(sid, base_dir, corr_method=method)\n",
    "                    if success:\n",
    "                        completed_methods += 1\n",
    "                        print(f\"‚úÖ Completed {method} for sub{sid}\")\n",
    "                    else:\n",
    "                        print(f\"‚ö†Ô∏è Skipped {method} for sub{sid} (missing files)\")\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Error in {method} for sub{sid}: {e}\")\n",
    "            \n",
    "            print(f\"\\nüìå Summary for sub{sid}: {completed_methods}/{len(methods_to_run)} methods completed.\")\n",
    "\n",
    "        print(f\"\\nüéâ Algorithm 2 batch processing complete for {total_subjects} subjects!\")\n",
    "        print(f\"Methods run: {', '.join(methods_to_run)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1cd889-37a2-48ef-948c-e1256d8197e4",
   "metadata": {},
   "source": [
    "### üñºÔ∏è Step 3: 3D Visualization of the Top DLPFC Cluster\n",
    "\n",
    "This step produces **high-quality 3D visualizations** of the optimal DLPFC subunit selected by Algorithm 2, rendered as smooth volumetric surfaces using `marching_cubes`.\n",
    "\n",
    "#### ‚ö†Ô∏è Critical Dependency:\n",
    "> **This visualization CANNOT run independently.**  \n",
    "> It **requires successful execution of both Algorithm 1 and Algorithm 2** for the same subject(s) and the **same correlation method** (e.g., `spearman`, `pearson`, etc.).\n",
    "\n",
    "Specifically, it expects the following files to exist in the `SAINT-PROTOCOL` folder:\n",
    "- `subXXX_SAINT_Protocol_Output_{method}.xlsx` ‚Üí **must contain the `Optimal_DLPFC` sheet** (generated by Algorithm 2)\n",
    "- `subXXX_cluster_voxel_coords_{method}.npz` ‚Üí **full voxel coordinates** (generated by Algorithm 1)\n",
    "\n",
    "If either file is missing or was generated with a different correlation method, the visualization will be **skipped** for that subject/method.\n",
    "\n",
    "#### üñåÔ∏è Visualization Details:\n",
    "- **Smooth mesh surfaces** (not point clouds) for anatomical realism\n",
    "- **Color scheme**:\n",
    "  - Light grey: Whole-brain background (low opacity)\n",
    "  - Gold: Full left DLPFC and sgACC masks\n",
    "  - Red: The **top-ranked DLPFC subunit** (high opacity, highlighted)\n",
    "- **Custom 3D camera angle** optimized for frontal view\n",
    "- **Clean annotation-based legend** (no extra markers in 3D space)\n",
    "- **Automatic fallback** to point-cloud rendering if mesh generation fails\n",
    "\n",
    "#### üíæ Output:\n",
    "- Saves a high-resolution PNG image:  \n",
    "  `subXXX_top_dlpfc_cluster_{method}.png`\n",
    "- Also displays an **interactive 3D figure** in the notebook/environment\n",
    "\n",
    "> üìå **Note**: Always verify that Algorithms 1 and 2 completed successfully before running this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cd5072-dc6f-4886-8b81-4d00e607b8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 3D Visualization: Top DLPFC Cluster from Algorithm 2\n",
    "# Uses Mesh3d + marching_cubes with annotation legend\n",
    "# Supports multiple correlation methods: 'spearman', 'pearson', 'kendall', 'crosscorr'\n",
    "# Saves output as PNG in SAINT-PROTOCOL folder\n",
    "# =========================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import nibabel as nib\n",
    "import os\n",
    "import re\n",
    "from skimage import measure  # For marching_cubes\n",
    "\n",
    "\n",
    "def find_mask_path(subject_dir, base_name):\n",
    "    \"\"\"Try bin first, then func.\"\"\"\n",
    "    bin_path = os.path.join(subject_dir, f\"{base_name}_bin.nii.gz\")\n",
    "    func_path = os.path.join(subject_dir, f\"{base_name}_func.nii.gz\")\n",
    "    if os.path.exists(func_path):\n",
    "        return func_path\n",
    "    elif os.path.exists(bin_path):\n",
    "        return bin_path\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def visualize_and_save_top_dlpfc(subject_id, base_dir=\"G:/SAINT/Subjects\", corr_method=\"spearman\"):\n",
    "    \"\"\"Visualize and save top DLPFC cluster using Mesh3d with annotation legend.\"\"\"\n",
    "    subject_dir = os.path.join(base_dir, f\"sub{subject_id}\")\n",
    "    output_dir = os.path.join(subject_dir, \"SAINT-PROTOCOL\")\n",
    "    \n",
    "    # Paths (dynamically based on corr_method)\n",
    "    excel_path = os.path.join(output_dir, f\"sub{subject_id}_SAINT_Protocol_Output_{corr_method}.xlsx\")\n",
    "    npz_path = os.path.join(output_dir, f\"sub{subject_id}_cluster_voxel_coords_{corr_method}.npz\")\n",
    "    fmri_path = os.path.join(subject_dir, \"filtered_func_data.nii.gz\")\n",
    "    \n",
    "    # Validate files\n",
    "    if not os.path.exists(excel_path):\n",
    "        print(f\"‚ö†Ô∏è Skipping sub{subject_id}: Excel output not found for method '{corr_method}'.\")\n",
    "        return False\n",
    "    if not os.path.exists(npz_path):\n",
    "        print(f\"‚ö†Ô∏è Skipping sub{subject_id}: .npz voxel data not found for method '{corr_method}'.\")\n",
    "        return False\n",
    "    if not os.path.exists(fmri_path):\n",
    "        print(f\"‚ö†Ô∏è Skipping sub{subject_id}: fMRI data not found.\")\n",
    "        return False\n",
    "    \n",
    "    # Load top cluster label from Excel\n",
    "    try:\n",
    "        df_algo2 = pd.read_excel(excel_path, sheet_name='Optimal_DLPFC')\n",
    "        top_cluster_label = int(df_algo2.iloc[0]['DLPFC_Cluster'])\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Skipping sub{subject_id}: Error reading Excel: {e}\")\n",
    "        return False\n",
    "\n",
    "    print(f\"‚úÖ Loading top DLPFC cluster {top_cluster_label} for sub{subject_id} (method: {corr_method})\")\n",
    "\n",
    "    # Load full voxel coordinates from .npz\n",
    "    try:\n",
    "        voxel_data = np.load(npz_path, allow_pickle=True)\n",
    "        dlpfc_voxel_arrays = voxel_data['dlpfc_voxel_coords']\n",
    "        dlpfc_labels = voxel_data['dlpfc_cluster_labels']\n",
    "        idx = np.where(dlpfc_labels == top_cluster_label)[0][0]\n",
    "        top_cluster_voxels = dlpfc_voxel_arrays[idx]\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Skipping sub{subject_id}: Error loading .npz: {e}\")\n",
    "        return False\n",
    "\n",
    "    # Load masks and data\n",
    "    dlpfc_mask_path = find_mask_path(subject_dir, \"l_DLPFC\")\n",
    "    sgacc_mask_path = find_mask_path(subject_dir, \"sgACC\")\n",
    "    if dlpfc_mask_path is None or sgacc_mask_path is None:\n",
    "        print(f\"‚ö†Ô∏è Skipping sub{subject_id}: Mask files not found.\")\n",
    "        return False\n",
    "\n",
    "    fmri_img = nib.load(fmri_path)\n",
    "    fmri_data = fmri_img.get_fdata()\n",
    "    brain_mask = np.any(fmri_data != 0, axis=3)\n",
    "    dlpfc_mask = nib.load(dlpfc_mask_path).get_fdata().astype(bool)\n",
    "    sgacc_mask = nib.load(sgacc_mask_path).get_fdata().astype(bool)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Build binary volumes for meshing\n",
    "    # -----------------------------\n",
    "    shape = brain_mask.shape\n",
    "    top_cluster_vol = np.zeros(shape, dtype=bool)\n",
    "    for coord in top_cluster_voxels:\n",
    "        top_cluster_vol[tuple(coord)] = True\n",
    "\n",
    "    # -----------------------------\n",
    "    # Create Mesh3d traces (for actual visualization)\n",
    "    # -----------------------------\n",
    "    traces = []\n",
    "\n",
    "    # 1. Whole brain mesh (background)\n",
    "    try:\n",
    "        verts, faces, _, _ = measure.marching_cubes(brain_mask.astype(float), level=0.5)\n",
    "        brain_mesh = go.Mesh3d(\n",
    "            x=verts[:, 0], y=verts[:, 1], z=verts[:, 2],\n",
    "            i=faces[:, 0], j=faces[:, 1], k=faces[:, 2],\n",
    "            color='lightgrey', opacity=0.08, name='Whole Brain', showlegend=False\n",
    "        )\n",
    "        traces.append(brain_mesh)\n",
    "    except RuntimeError:\n",
    "        brain_iso = go.Isosurface(\n",
    "            x=np.arange(shape[0]), y=np.arange(shape[1]), z=np.arange(shape[2]),\n",
    "            value=brain_mask.astype(float),\n",
    "            isomin=0.5, isomax=1.0,\n",
    "            opacity=0.08, colorscale=[[0, 'lightgrey'], [1, 'lightgrey']],\n",
    "            showscale=False, showlegend=False, name='Whole Brain'\n",
    "        )\n",
    "        traces.append(brain_iso)\n",
    "\n",
    "    # 2. Full DLPFC mask mesh ‚Äî yellow\n",
    "    if np.any(dlpfc_mask):\n",
    "        try:\n",
    "            verts, faces, _, _ = measure.marching_cubes(dlpfc_mask.astype(float), level=0.5)\n",
    "            dlpfc_mesh = go.Mesh3d(\n",
    "                x=verts[:,0], y=verts[:,1], z=verts[:,2],\n",
    "                i=faces[:,0], j=faces[:,1], k=faces[:,2],\n",
    "                color='gold', opacity=0.5, name='DLPFC (full mask)', showlegend=False\n",
    "            )\n",
    "            traces.append(dlpfc_mesh)\n",
    "        except RuntimeError:\n",
    "            pass\n",
    "\n",
    "    # 3. Full sgACC mask mesh ‚Äî gold\n",
    "    if np.any(sgacc_mask):\n",
    "        try:\n",
    "            verts, faces, _, _ = measure.marching_cubes(sgacc_mask.astype(float), level=0.5)\n",
    "            sgacc_mesh = go.Mesh3d(\n",
    "                x=verts[:,0], y=verts[:,1], z=verts[:,2],\n",
    "                i=faces[:,0], j=faces[:,1], k=faces[:,2],\n",
    "                color='gold', opacity=0.5, name='sgACC (full mask)', showlegend=False\n",
    "            )\n",
    "            traces.append(sgacc_mesh)\n",
    "        except RuntimeError:\n",
    "            pass\n",
    "\n",
    "    # 4. Top DLPFC cluster ‚Äî red (highlighted)\n",
    "    if np.any(top_cluster_vol):\n",
    "        try:\n",
    "            verts, faces, _, _ = measure.marching_cubes(top_cluster_vol.astype(float), level=0.5)\n",
    "            top_mesh = go.Mesh3d(\n",
    "                x=verts[:,0], y=verts[:,1], z=verts[:,2],\n",
    "                i=faces[:,0], j=faces[:,1], k=faces[:,2],\n",
    "                color='#d7191c', opacity=0.9, name=f'DLPFC - Top Cluster {top_cluster_label}', showlegend=False\n",
    "            )\n",
    "            traces.append(top_mesh)\n",
    "        except RuntimeError:\n",
    "            # Fallback to scatter only for top cluster if meshing fails\n",
    "            top_scatter = go.Scatter3d(\n",
    "                x=top_cluster_voxels[:,0], y=top_cluster_voxels[:,1], z=top_cluster_voxels[:,2],\n",
    "                mode='markers',\n",
    "                marker=dict(size=3, color='#d7191c', opacity=0.9),\n",
    "                name=f'DLPFC - Top Cluster {top_cluster_label} (fallback)',\n",
    "                showlegend=False\n",
    "            )\n",
    "            traces.append(top_scatter)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Final figure ‚Äî NO AXES, NO GRID, NO LABELS\n",
    "    # -----------------------------\n",
    "    fig = go.Figure(data=traces)\n",
    "    fig.update_layout(\n",
    "        title={\n",
    "            'text': f\"Top DLPFC Cluster from Algorithm 2<br><sub>Method: {corr_method} | Cluster {top_cluster_label} | Subject sub{subject_id}</sub>\",\n",
    "            'x': 0.02,\n",
    "            'xanchor': 'left',\n",
    "            'font': {'size': 16}\n",
    "        },\n",
    "        scene=dict(\n",
    "            xaxis=dict(visible=False),\n",
    "            yaxis=dict(visible=False),\n",
    "            zaxis=dict(visible=False),\n",
    "            aspectmode='data',\n",
    "            camera=dict(\n",
    "                eye=dict(x=-0.5, y=1.5, z=1),\n",
    "                up=dict(x=0, y=1, z=1),\n",
    "                center=dict(x=0, y=0, z=0)\n",
    "            )\n",
    "        ),\n",
    "        width=1100, height=950,\n",
    "        showlegend=False,\n",
    "        margin=dict(l=0, r=0, t=50, b=0)\n",
    "    )\n",
    "\n",
    "    # -----------------------------\n",
    "    # ADD CUSTOM LEGEND AS ANNOTATION (NO SCATTER3D!)\n",
    "    # -----------------------------\n",
    "    fig.add_annotation(\n",
    "        x=0.02, y=0.98,\n",
    "        text=\"<b>Legend:</b><br>\"\n",
    "             \"<span style='color:lightgrey'>‚óè</span> Brain (background)<br>\"\n",
    "             \"<span style='color:gold'>‚óè</span> DLPFC (full mask)<br>\"\n",
    "             \"<span style='color:gold'>‚óè</span> sgACC (full mask)<br>\"\n",
    "             \"<span style='color:#d7191c'>‚óè</span> DLPFC - Top Cluster {} ({} voxels)\".format(top_cluster_label, len(top_cluster_voxels)),\n",
    "        showarrow=False,\n",
    "        xref=\"paper\", yref=\"paper\",\n",
    "        align=\"left\",\n",
    "        bgcolor=\"rgba(255,255,255,0.8)\",\n",
    "        bordercolor=\"black\",\n",
    "        borderwidth=1,\n",
    "        font=dict(size=14)\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # SAVE IMAGE with method name\n",
    "    # -------------------------\n",
    "    image_base = os.path.join(output_dir, f\"sub{subject_id}_top_dlpfc_cluster_{corr_method}\")\n",
    "    \n",
    "    try:\n",
    "        fig.write_image(f\"{image_base}.png\", scale=2)\n",
    "        print(f\"üíæ Image saved to: {image_base}.png\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Warning: Could not save image (install 'kaleido'): {e}\")\n",
    "\n",
    "    # Also show interactively\n",
    "    fig.show()\n",
    "    return True\n",
    "\n",
    "# -------------------------\n",
    "# Main execution logic\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # ‚öôÔ∏è CONFIGURATION\n",
    "    \n",
    "    # Choose mode:\n",
    "    run_all_methods = True  # Set to False to run only one method\n",
    "    corr_method = \"spearman\"  # Used only if run_all_methods = False\n",
    "\n",
    "    # List of all supported correlation methods\n",
    "    all_methods = [\"spearman\", \"pearson\", \"kendall\", \"crosscorr\"]\n",
    "\n",
    "    # Decide which methods to run\n",
    "    methods_to_run = all_methods if run_all_methods else [corr_method]\n",
    "\n",
    "    # Single subject or batch?\n",
    "    subject_id = None  # Set to e.g., \"001\" for single subject; None for batch\n",
    "\n",
    "    if subject_id is not None:\n",
    "        for method in methods_to_run:\n",
    "            print(f\"\\nüñºÔ∏è  Visualizing top DLPFC for sub{subject_id} with method: {method}\")\n",
    "            success = visualize_and_save_top_dlpfc(subject_id, base_dir, corr_method=method)\n",
    "            if success:\n",
    "                print(f\"‚úÖ Visualization completed for {method}!\")\n",
    "            else:\n",
    "                print(f\"‚ùå Failed for {method}.\")\n",
    "    else:\n",
    "        subject_folders = [f for f in os.listdir(base_dir) if re.match(r'^sub\\d+$', f)]\n",
    "        subject_ids = sorted([f.replace('sub', '') for f in subject_folders], key=int)\n",
    "        total_subjects = len(subject_ids)\n",
    "\n",
    "        print(f\"Found {total_subjects} subjects. Starting batch visualization...\\n\")\n",
    "        \n",
    "        for sid in subject_ids:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Visualizing subject: sub{sid}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            completed_methods = 0\n",
    "            for method in methods_to_run:\n",
    "                try:\n",
    "                    print(f\"\\n‚Üí Running method: {method}\")\n",
    "                    success = visualize_and_save_top_dlpfc(sid, base_dir, corr_method=method)\n",
    "                    if success:\n",
    "                        completed_methods += 1\n",
    "                        print(f\"‚úÖ Completed {method} for sub{sid}\")\n",
    "                    else:\n",
    "                        print(f\"‚ö†Ô∏è Skipped {method} for sub{sid}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Error in {method} for sub{sid}: {e}\")\n",
    "            \n",
    "            print(f\"\\nüìå Summary for sub{sid}: {completed_methods}/{len(methods_to_run)} methods visualized.\")\n",
    "\n",
    "        print(f\"\\nüéâ Batch visualization complete for {total_subjects} subjects!\")\n",
    "        print(f\"Methods visualized: {', '.join(methods_to_run)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997de6a2-4a46-48bc-bb93-102000761ba4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
